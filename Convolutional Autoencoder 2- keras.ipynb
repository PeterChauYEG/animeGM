{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnimeGM - Autoencoder - Keras\n",
    "GOAL: Generate new anime-style images\n",
    "\n",
    "View model: `$ tensorboard --logdir=autoencoder`\n",
    "\n",
    "Methodology:\n",
    "1. Build a dataset of images suitable for out needs\n",
    "2. Build a generative model.\n",
    "3. Train the model\n",
    "4. Transfer style to a photo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D, Reshape, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir, path\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "from skimage import color, data, transform\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf image preprocessing\n",
    "# load, decoded and resize\n",
    "def parse_image(flatten, n_channels, path, resize_dim):\n",
    "    \"\"\"parses an image and converts it to the type we want.\n",
    "    flatten: to flatten the image into a vector or not.\n",
    "    n_channels: number of color channels to set (color / grayscale) - 1 or 3.\n",
    "    path: path to the image.\n",
    "    resize_dim: the number of pixels the image should be per size.\n",
    "    \n",
    "    return: the image as a vector or matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    image_string = tf.read_file(path)\n",
    "    \n",
    "    # decode\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    \n",
    "    # check for 1 channel\n",
    "    # if so, grayscale\n",
    "    if (n_channels == 1):\n",
    "        image = tf.image.rgb_to_grayscale(image)\n",
    "        \n",
    "    # resize the image\n",
    "    image = tf.image.resize_images(image, [resize_dim, resize_dim])\n",
    "    \n",
    "    # feature normalize the image\n",
    "    image = tf.divide(image, 255)\n",
    "    \n",
    "    # check if we want the image flattened\n",
    "    # if so, flatten the image\n",
    "    if flatten == True:\n",
    "        image = tf.reshape(image, [-1])\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(paths, iter, mode, sess):   \n",
    "    \"\"\"Creates a data generator.\n",
    "    paths: image paths to feed the generator with.\n",
    "    iter: TF dataset iterator.\n",
    "    mode: the mode we want (train or test).\n",
    "    sess: instance of a TF session.\n",
    "    \n",
    "    return: an image batch\n",
    "    \"\"\"\n",
    "    \n",
    "    next_batch = iter.get_next()\n",
    "    \n",
    "    # Initialize an iterator over a dataset.\n",
    "    sess.run(iter.initializer, feed_dict={image_paths: paths})\n",
    "\n",
    "    # yield the next batch\n",
    "    while 1:\n",
    "        batch = sess.run(next_batch)\n",
    "        \n",
    "        # check for mode\n",
    "        if mode == 'train':\n",
    "            result = batch, batch\n",
    "        elif mode == 'test':\n",
    "            result = batch\n",
    "            \n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_image(image, n_channels, resize_dim):\n",
    "    \"\"\"Shows an image.\n",
    "    image: image data to show.\n",
    "    n_channels: number of color channels to set (color / grayscale) - 1 or 3.\n",
    "    resize_dim: the number of pixels the image should be per size.\n",
    "    \"\"\"\n",
    "    \n",
    "    # check for 1 channel\n",
    "    # if so, grayscale\n",
    "    if (n_channels == 1):\n",
    "        image = image.reshape((resize_dim, resize_dim))\n",
    "    else:\n",
    "        image = image.reshape((resize_dim, resize_dim, n_channels))\n",
    "\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_images(images, n_channels, resize_dim):\n",
    "    \"\"\"Shows a set of images.\n",
    "    images: a set of image data to show.\n",
    "    n_channels: number of color channels to set (color / grayscale) - 1 or 3.\n",
    "    resize_dim: the number of pixels the image should be per size.\n",
    "    \"\"\"\n",
    "    \n",
    "   # get the number of images\n",
    "    n_images = images.shape[0]\n",
    "    \n",
    "    if n_images == 1:\n",
    "        show_image(images[0], n_channels, resize_dim)\n",
    "        return\n",
    "    \n",
    "    # get the square root of the number of images\n",
    "    squareroot = int(np.ceil(np.sqrt(n_images)))\n",
    "    \n",
    "    # size of the figure\n",
    "    figsize = (10, 10)\n",
    "    \n",
    "    # configure subplots\n",
    "    fig, ax = plt.subplots(squareroot, squareroot, figsize=figsize,\n",
    "                           subplot_kw=dict(xticks=[], yticks=[]))\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "        # exit if the index of subplots is greater than amount of images\n",
    "        if i > n_images - 1:\n",
    "            return\n",
    "        \n",
    "        image = images[i]\n",
    "        \n",
    "        # check for 1 channel\n",
    "        # if so, grayscale\n",
    "        if (n_channels == 1):\n",
    "            image = image.reshape((resize_dim, resize_dim))\n",
    "        else:\n",
    "            image = image.reshape((resize_dim, resize_dim, n_channels))\n",
    "\n",
    "        im = axi.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(image, n_channels, resize_dim):\n",
    "    \"\"\"Gray scales an image (if enough channels) and resizes it aswell.\n",
    "    image: image data.\n",
    "    resize_dim: new image dimension.\n",
    "    \n",
    "    return grayscaled, resized image\n",
    "    \"\"\"\n",
    "    \n",
    "    # check for 1 channel\n",
    "    # if so, grayscale\n",
    "    if (n_channels == 1):\n",
    "        image = color.rgb2gray(image)\n",
    "    \n",
    "    # Resize the image\n",
    "    image = transform.resize(image, (resize_dim, resize_dim))\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data dir\n",
    "dir_name = 'danbooru-small'\n",
    "\n",
    "# log dir\n",
    "log_dir = 'log'\n",
    "\n",
    "# checkpoint path\n",
    "checkpoint_path = 'checkpoints/weights.best.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "resize_dim = 104\n",
    "n_channels = 3\n",
    "n_features = resize_dim * resize_dim * n_channels\n",
    "\n",
    "# model type: flatten false for cnn\n",
    "flatten = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of examples\n",
    "n_test = 10 # for reconstruction - subset of validation\n",
    "n_train = 3691 # for training\n",
    "n_validation = int(n_train * 0.2) # for training validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters~~~~\n",
    "batch_size = 50\n",
    "n_batches = int(n_train / batch_size)\n",
    "n_validation_batches = int(n_validation / batch_size)\n",
    "\n",
    "n_epochs = 500\n",
    "\n",
    "# for a cnn layer between encoder / decoder\n",
    "# cnn_dim = int(resize_dim / 2 / 2 / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a dataset\n",
    "Pull in an existing dataset and modify it for my needs.\n",
    "\n",
    "Lets start with the danbooru 2017 anime image dataset. Now this dataset is huge. So we are only going to use 1 torrent of the SFW subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filenames and join paths as np array\n",
    "base_filenames = listdir(dir_name)\n",
    "filename_paths = [path.join(dir_name, base_filename) for base_filename in base_filenames]\n",
    "filename_paths = np.array(filename_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create paths for train and test\n",
    "test_paths = filename_paths[:n_test]\n",
    "train_paths = filename_paths[:-n_validation][:n_train]\n",
    "validation_paths = filename_paths[:n_validation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    # create a placeholder for image_paths\n",
    "    image_paths = tf.placeholder(tf.string, shape=[None])\n",
    "\n",
    "    # create a dataset from the paths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    dataset = dataset.map(lambda image_paths: parse_image(flatten, n_channels, image_paths, resize_dim))\n",
    "    dataset = dataset.shuffle(buffer_size=n_train)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=batch_size)\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "    # iterators\n",
    "    iter = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a generative model\n",
    "Lets build an autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input placeholder\n",
    "inputs = Input(shape=(resize_dim, resize_dim, n_channels))\n",
    "\n",
    "# encoded the input with convolutions and pooling\n",
    "x = Conv2D(100, (3, 3), activation='relu', name='encoder_0', padding='same')(inputs)\n",
    "x = MaxPooling2D((2, 2), padding='same', name='pool_0')(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', name='encoder_1', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same', name='pool_1')(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', name='encoder_2', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same', name='pool_2')(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', name='encoder_3', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same', name='pool_3')(x)\n",
    "\n",
    "# decoded the compression and upsampling\n",
    "x = Conv2D(100, (3, 3), activation='relu', name='decoder_0', padding='same')(x)\n",
    "x = UpSampling2D((2, 2), name='upsample_0')(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', name='decoder_1', padding='same')(x)\n",
    "x = UpSampling2D((2, 2), name='upsample_1')(x)\n",
    "x = Conv2D(100, (3, 3), activation='relu', name='decoder_2', padding='same')(x)\n",
    "x = UpSampling2D((2, 2), name='upsample_2')(x)\n",
    "decoded = Conv2D(n_channels, (3, 3), activation='sigmoid', name='decoder_3', padding='same')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model with cost and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps inputs to its reconstructions\n",
    "autoencoder = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model weights if they exist\n",
    "if path.isfile(checkpoint_path):\n",
    "    autoencoder.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(loss='binary_crossentropy', \n",
    "                    optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a session to use the graph\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "# create instances of the generator\n",
    "training_batch_generator = data_generator(train_paths, iter, 'train', sess)\n",
    "validation_batch_generator = data_generator(validation_paths, iter, 'train', sess)\n",
    "test_batch_generator = data_generator(test_paths, iter, 'test', sess)\n",
    " \n",
    "# create checkpoint for weights\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, verbose=1, \n",
    "                             save_best_only=True, save_weights_only=True,\n",
    "                             monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train\n",
    "autoencoder.fit_generator(training_batch_generator, \n",
    "                          steps_per_epoch=n_batches,\n",
    "                          epochs=n_epochs,\n",
    "                          validation_data=validation_batch_generator,\n",
    "                          validation_steps=n_validation_batches,\n",
    "                          callbacks=[checkpoint, TensorBoard(log_dir=log_dir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reconstructions  \n",
    "# get batch\n",
    "test_batch = next(test_batch_generator)\n",
    "\n",
    "# show the test images\n",
    "show_images(test_batch, n_channels, resize_dim)\n",
    "\n",
    "# create reconstructions\n",
    "reconstructions = autoencoder.predict(test_batch)\n",
    "\n",
    "# show the reconstructioned test images\n",
    "show_images(reconstructions, n_channels, resize_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply model to a real image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab an image\n",
    "transfer_image = data.astronaut()\n",
    "\n",
    "# resize the image and put it into gray scale\n",
    "resized_transfer_image = transform_image(transfer_image, n_channels, resize_dim)\n",
    "\n",
    "# batch it\n",
    "reshaped_transfer_image = resized_transfer_image.reshape(-1, resize_dim, resize_dim, n_channels)\n",
    "\n",
    "# display an image\n",
    "show_image(resized_transfer_image, n_channels, resize_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the image through the model\n",
    "test_y = autoencoder.predict(reshaped_transfer_image)\n",
    "\n",
    "# view the resulting image\n",
    "show_image(test_y, n_channels, resize_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
