{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnimeGM - Style Transfer\n",
    "GOAL: Generate new anime-style images\n",
    "\n",
    "View model: `$ tensorboard --logdir=autoencoder`\n",
    "\n",
    "Methodology:\n",
    "1. Load the images and preprocess them to a consistent size, and shape\n",
    "2. Build models for a, p, x using a pretrained model (VGG16)\n",
    "3. Construct a placeholder array for style weights if the style layers\n",
    "4. Extract feature representations to construct p and a, for each selected layer\n",
    "5. Optimize with the BFGS algorithm\n",
    "\n",
    "Important Functions:\n",
    "1. Define a function to calculate the content loss\n",
    "2. Define a function to calculate the style loss\n",
    "3. Define a function to calculate the total loss\n",
    "4. Define a function to calculate the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.keras.api.keras.backend as K\n",
    "from tensorflow.contrib.keras.api.keras.applications import vgg19\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.misc import imresize, imsave\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import seaborn as sns; sns.set()\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_image(image, resize_dim):\n",
    "    \"\"\"Shows an image.\n",
    "    image: image data to show.\n",
    "    resize_dim: the number of pixels the image should be per size.\n",
    "    \"\"\"\n",
    "    image = image.reshape((resize_dim, resize_dim, 3))\n",
    "\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img_path, resize_dim):\n",
    "    # load the image from file system\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path)\n",
    "\n",
    "    # convert it to an array\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "    # resize it\n",
    "    img = imresize(img, (resize_dim, resize_dim, 3))\n",
    "\n",
    "    # cast the image to a float64 \n",
    "    img = img.astype('float32')\n",
    "#     img = img.astype('float64')\n",
    "\n",
    "    # add a batch number\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    # add the mean pixel values\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    \n",
    "    # show the image (it will be clipped)\n",
    "    show_image(img, resize_dim)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(content, gen):\n",
    "    \"\"\"get the Euclidean distance between outputs of more for the content \n",
    "    and generated image at a specific layer\"\"\"\n",
    "    # why sum of squared errors?\n",
    "    # why not the mean squared errors?\n",
    "    return K.sum(K.square(gen - content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    \"\"\"calculates the gram matrix.\n",
    "    dot product of the flattened feature map and transposed \n",
    "    flatten feature map\"\"\"\n",
    "    assert K.ndim(x) == 3\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(style, gen, resize_dim, channels):\n",
    "    \"\"\"calculates the Euclidean distance between gram matrices of \n",
    "    the feature maps of the inputs\"\"\"\n",
    "    assert K.ndim(style) == 3\n",
    "    assert K.ndim(gen) == 3\n",
    "    S = gram_matrix(style)\n",
    "    G = gram_matrix(gen)\n",
    "    size = resize_dim * resize_dim\n",
    "    \n",
    "    # divide by $ * size^2 * channels^2\n",
    "    return K.sum(K.square(S - G) / (4. * (channels ** 2) * (size ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss_and_grads(x, f_output, resize_dim, channels):\n",
    "    x = x.reshape((1, resize_dim, resize_dim, channels))\n",
    "    outs = f_output([x])\n",
    "    loss_value = outs[0]\n",
    "    if len(outs[1:]) == 1:\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "    else: \n",
    "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "    return loss_value, grad_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(object):\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_value = None\n",
    "        \n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_value = eval_loss_and_grads(x, f_output, resize_dim, channels)\n",
    "        self.loss_value = loss_value\n",
    "        self.grads_value = grad_value\n",
    "        return self.loss_value\n",
    "    \n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grads_values = np.copy(self.grads_value)\n",
    "        self.loss_value = None\n",
    "        self.grads_value = None\n",
    "        return grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(img, resize_dim, channels):\n",
    "    img = img.reshape((resize_dim, resize_dim, channels))\n",
    "    \n",
    "    # Remove zero-center by mean pixel\n",
    "    img[:, :, 0] += 103.939\n",
    "    img[:, :, 1] += 116.779\n",
    "    img[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    img = img[:, :, ::-1]\n",
    "    img = np.clip(img, 0, 255).astype('uint8')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data dir\n",
    "content_path = 'images/content.png'\n",
    "style_path = 'images/style2.jpg'\n",
    "\n",
    "# data manipulation\n",
    "resize_dim = 128\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "content_weight = 0.5\n",
    "style_weight = 0.5\n",
    "iterations = 10\n",
    "loss_iterations = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2bf29f754845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_VISIBLE_DEVICES\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load images\n",
    "content image = p  \n",
    "style image = a  \n",
    "generated image = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img = preprocess(content_path, resize_dim)\n",
    "style_img = preprocess(style_path, resize_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images into vars\n",
    "content_img_var = K.variable(content_img)\n",
    "style_img_var = K.variable(style_img)\n",
    "gen_img = K.placeholder(shape=(1, resize_dim, resize_dim, 3))\n",
    "\n",
    "# batch them all together\n",
    "input_tensor = K.concatenate([content_img_var, style_img_var, gen_img], axis=0)\n",
    "\n",
    "# load the trained VGG19 model with imagenet weights, and specify an input tensor\n",
    "model = vgg19.VGG19(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "\n",
    "# make a dictionary of all the names and outputs of the model layers\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the feature map\n",
    "loss = 0\n",
    "\n",
    "# higher layers in the model are better for overall shapes\n",
    "layer_features = outputs_dict['block5_conv2']\n",
    "\n",
    "# index 0 because of our input tensor batching\n",
    "content_img_features = layer_features[0, :, :, :] \n",
    "\n",
    "# index 2 because of our input tensor batching\n",
    "gen_img_features = layer_features[2, :, :, :]\n",
    "\n",
    "# use content_weight to give weight to the amount to use in the mix\n",
    "loss += content_weight * content_loss(content_img_features, gen_img_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab layer names for our style loss\n",
    "# notice that it's the first conv layer of each block\n",
    "feature_layer_names = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "\n",
    "# get loss for each feature layer\n",
    "for name in feature_layer_names:\n",
    "    # grab the layer\n",
    "    layer_features = outputs_dict[name]\n",
    "    \n",
    "    # index 1 because of our input tensor batching\n",
    "    style_features = layer_features[1, :, :, :]\n",
    "    \n",
    "    # index 2 because of our input tensor batching\n",
    "    gen_img_features = layer_features[2, :, :, :]\n",
    "    \n",
    "    # get the style loss\n",
    "    s1 = style_loss(style_features, gen_img_features, resize_dim, channels)\n",
    "    \n",
    "    # use style_weight to give weight to the amount to use in the mix\n",
    "    # scale it based it on the number of faeture layers\n",
    "    loss += (style_weight / len(feature_layer_names)) * s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the gradients and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = K.gradients(loss, gen_img)\n",
    "\n",
    "outputs = [loss]\n",
    "\n",
    "if isinstance(grads, (list, tuple)):\n",
    "    outputs += grads\n",
    "else:\n",
    "    outputs.append(grads)\n",
    "    \n",
    "f_output = K.function([gen_img], outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "\n",
    "x = content_img\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('Step {}'.format(i))\n",
    "    \n",
    "    # run the L-BFGS optimizer\n",
    "    # this is the least memory intensive\n",
    "    # the more iterations the better\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxiter=loss_iterations)\n",
    "    print('     loss: {}'.format(min_val))\n",
    "\n",
    "    # save img\n",
    "    img = deprocess_image(x, resize_dim, channels)\n",
    "    show_image(img, resize_dim)\n",
    "    imsave('output/img{}.jpg'.format(i), img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
